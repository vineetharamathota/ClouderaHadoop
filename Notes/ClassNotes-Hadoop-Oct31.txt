
  Curriculum:
  --------------------------------------
  Big Data & Hadoop Intro
  Hadoop Architecture - HDFS & MapReduce
  HDFS 
  MapReduce  
  Hive 
  Sqoop
  Oozie
  HBase 
  Kafka
  -------------------------------------

   
  What is Big Data ? 
  ------------------
 
   -> A collection of of data that is huge in volume and growing exponentially with time.

   -> Such data is so large and complex that the traditional data management tools are not
      able to store or process it efficiently.    

      -> Traditional data management tools: are single server based technology.
   
      -> We have move to cluster based architectures and use the cumulative resources of
         all the nodes in the cluster to store and process you data.   

   -> Problems:  Storage & Processing
    
      
   Cluster => A unified storage and computational entity containing many nodes. 
     

  Types of Data
  -------------
    -> Structured Data
    -> Unstructured Data
    -> Semi-Structured  

  
  Characteristics
  ---------------
     1. Volume
     2. Velocity
     3. Variety 
     4. Variability / Veracity  


  
   Hadoop
   ======

    Is an open-source framework to efficeiently store and process big data using a 
    cluster made of commodity hardware. 

    It provides a solution to big data problems by providing a platform to do:

    -> Distributed Storage
    -> Distributed Parallel Process

    1. HDFS (Hadoop Distributed File System) -> Storage Solution
    2. MapReduce                             -> Procesding Solution



  Hadoop Distributions
  ====================

       -> Cloudera
       -> Hortonworks
       -> MapR

       -> Hadoop Insights (Azure)
       -> Elastic Map Reduce (EMR) - AWS

   

    HDFS
    -----
      
      -> The is data stored in blocks of 128 MB each (max-size of the block is 128 MB)
      -> These blocks are distributed into a lot of nodes in the cluster
      -> Each block belongs only to one file.
      -> 10250 MB  => 128 MB * 80 blocks + 10 MB block (81 blocks)


   MapReduce
   ---------

      -> Launches multiple 'mapper' simultaneously on all the blocks
      -> Collect the intermediate output produced by each mappers
      -> Aggregate all the intermediate output
      -> Send this aggregated output to  a 'reducer' task that produces the final output.

  
   HDFS Architecture (Storage)
   ---------------------------

    -> Hadoop follows a master-slave architecture


    HDFS Daemons 
    ------------

        => Name Node (daemon) runs on the master-machine

             -> NN receives heart-beats from the DNs and store the info of the block reports 
                in two files:
 
                -> Edit Log (Dynamic) - maintained in RAM
                    -> records all the block info in the edit-log

                -> FS Image (Static) - maintained on disk
                     -> complete storage information since th starting of the cluster
                        upto the last point time.


        => Data Node (daemon) runs on all the slave machine
             -> Maintain info about all the blocks, file, permissions, owner ...
             -> Send heart beats to the NN onec every three seconds.
             -> They send "block report" in the heart-beat
 
        => Secondary Name Node (SNN)
             -> Is a master process that runs on a separate machine and is resposible for check-pointing 

             Checkpointing:
                  -> SNN copies the EditLog and FSImage  
                  -> Merges the edit-log file with the FSImage file
                  -> The updated FSIMage file is copied back to the Master Node.

                   -> During checkpointing the master-node creates a blank edit-log file.
                   -> This new edit-log file will keep on recording the blobk-reports.

         => Balancer
               -> It will maintain the replication-factor of the blocks
               -> If a datanode is down, it will create an additional replicas of the blocks contained
                  in the failed datanode.
               -> Will remove the over-replicated blocks, when the failed DN comes back up.

     
     HDFS 
     ----
      Home Path on Linux: /home/cloudera
      Home Path on HDFS:  /user/cloudera


      Two syntaxes: $hdfs dfs -<command-name>
                    $hadoop fs -<command-name>

      1. hadoop fs -ls
      2. hadoop fs -ls <directory-name>
         hadoop fs -ls -R <directory-name>   (recursive listing)

      3. hadoop fs -mkdir <dir-name>

      4. hadoop fs -copyFromLocal <local-file> <hdfs-path>
         hadoop fs -put <local-file> <hdfs-path>

      5. hadoop fs -copyToLocal <hdfs-path> <local-dir>
         hadoop fs -get <hdfs-path> <local-dir>

      6. hadoop fs -cp wordcount_input.txt synechron2   (Copy with HDFS directories)

      7. hadoop fs -rmdir synechron2  (remove a empty directory)

      8.  hadoop fs -rm synechron2/<filepath> (to remove files)
          hadoop fs -rm -R synechron2         (remove a non-empty directory)

      hadoop fs -get <filename> <local-dir>


  MapReduce
  =========
     
     Hadoop 2.0  =>  HDFS        => Storage 
                     MapReduce   => Execution Engine
                     YARN        => Resource Manager

     
    Processing Daemons => Resource Manager (one per cluster)
    (YARN)                Node Manager (one per node)
                          Application Master (one per application)


    MapReduce  ===> MapReduce Execution Framework  (How the code is executed)
                    MapReduce Programming API       (Developer)
                            
   
   MApreduce APIs
   ---------------

      org.apache.hadoop.io          => Writable Data types
      org.apache.hadoop.mapreduce   => Framework classes
      org.apache.hadoop.conf        => Configurations
      org.apache.hadoop.util        => Utilities


   MapReduce Programming API
   -------------------------

    1. Driver Program
    2. Mapper Program
    3. Reducer Program

    
   MapReduce Execution Framework 
   -----------------------------


   Map Phase
   ---------

    1. Create Input Splits (IS) from the blocks
    2. Record Reader reads records from the IS and create a (K,V) pair and calls the map() method
        if the input split has 100 records, 100 times map() method will be called
    3. map() method will process the record and writes the output into a context buffer
    4. The data from the context buffer will be spilled onto disk as intermediate files.


    (0, hadoop hive hbase)      => map => (hadoop, 1) (hive, 1) (hbase, 1)    
    (75, pig spark mapreduce)   => map => (pig, 1) (spark, 1) (mapreduce, 1)
    (120, hadoop spark hive)    => map => (hadoop, 1) (spark, 1) (hive, 1)


  Shuffle and Sort Phase
  ----------------------
    
     5. The data from all the intermediate files from across various executors where mappers 
        are running is collected and the values of each unique key is aggregated into a set 
        of partitions based on the number of reducers requested in the program. 


        (hadoop, 1) (hive, 1) (hbase, 1)    --> (hadoop, (1,1) ) (hive, (1,1)) (hbase, (1)) ==> R0
        (pig, 1) (spark, 1) (mapreduce, 1)  --> (pig, (1) ) (spark, (1,1)) (mapreduce, (1)) ==> R1
        (hadoop, 1) (spark, 1) (hive, 1)

  
  Reducer Phase
  -------------
     6. The input from SS phase will be in the form of (K, Iterable[V])
     7. These output (K, V) are given as input to "reduce" method. 
     8. reduce method will aggregate the final results and write the output to HDFS

     input: (hadoop, (1,1) )       output:  (hadoop, 2)

     Text, LongWritable, Text, LongWritable


     Flow:
        Map Phase  -> Combiner Phase (optional) -> Shuffle & Sort -> Partitioner (optioner) -> Reducer


  Submitting a Map Reduce Program
  --------------------------------
  hadoop jar <jar-file-name> <driver-class-qualified-name> <command-line-arguments>   
  hadoop jar /home/cloudera/MRWordCount.jar wordcount.WordCountJob wordcount_input.txt wcoutput

  




