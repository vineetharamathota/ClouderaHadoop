 
   Kafka
   -----

   Batch Processing
    
       => Data        :  100 MB 
       => Processing  : Started at 9:00 AM and completed at 9:10 AM


   Streaming Data Processing (Real time data processing)
 
       => Data       : Continuous
       => Processing : Continuous and Real-time.


   Data Pipeline

   Web Applications                         Database
   Kiosks/ATM                               Chat Application
   Mobile App				    Security Systems
  
   

  Kafka
  ------

      => Is a pub/sub messaging engines
      => Is a streaming processing engines (can write streaming application)

  
  Kafka Components
  ----------------
      => Kafka runs as a cluster containing many nodes


     1. Broker
            => Each node in a Kafka Cluster is called a "broker"

     2. Zookeeper
            => ZK provides the coordination service for Kafka to form a cluster
            => Kafka brokers depend on Zookeeper broker for management and coordination services.

     3. Topic
            => Is a category/feed, which stores a specific types of messages are published.
            => Ex: Orders, Transactions, Logs ...
            => Each topic is a distributed commit log. 
            => Distributed as "partitions" across multiple brokers.

     4. Partitons
            => Topics are broken into ordered commit logs called partitions. 
          
     5. Producer
            => Is an application that produces messages to a topic.

     6. Consumer
            => Is an application that subcribes to a topic and consumes messages from the topic.
   
  
    Producer :
         B0: ............         ===> P0
         B1: ............         ===> P1
         B2: ............         ===> P2
   
        
 Kafka Producers
 ===============

    How messages are sent to a specific partition:

      1. We can produce a message to a specific partition, then the message is sent to 
         the specifed partition.

      2. We can attach partitioner while producing a message. Then the partitioner's logic is
         used to determine which partition a specific message goes to.

      3. If no partition is specified and no partitioner is attached, then the message's partition
         will be determined by the message "key" (default hash partitioner is user)

      4. If the key is not present in the message, them messages are randomly distributed across
         partitions. 


 

  Kafka APIs
  ---------

   1. Producer API   -> producer application

         1. Fire and Forget  -> We don't receive any acknowlegments from the broker.

                producer.send(  new ProducerRecord(producer.send(topic, key, value)) )

         2. Synchronous      -> We wait for acknowlegments for each message
		RecordmetaData meta = producer.send( new ProducerRecord(producer.send(topic, key, value)) )
                                              .get()

         3. Asynchronous     -> We do not wait for acknowlegments.
                                Acknowledgments are handled by a call-back method.

               producer.send( new ProducerRecord(producer.send(topic, key, value)), <callback> )



   2. Consumer API   -> consumer application

          1. Consumer subscribes to a topic (or a specific partition)
          2. It will poll for the records from th the topic in a "poll loop"
          3. The pool returns a collection of records
          4. Commits the offset of the records to the __consumer_offset topic automatically (auto-commit-offsets)
            
  

   3. Streaming API  -> Event stream applications 
                     -> Reads from a topic -> process -> write to another topic

    
   4. Connect API    -> transferring data across mutpliple sources and destinations automatically. 

        Oracle <---  [source connector] -> [connectors] <- [sink connector]   ---> ElasticSearch       
  
      
  Rebalancing in Kafka:

     => Rebalancing happens when:

           1. A new consumer is added to the consumer group.
           2. An existing consumer goes out (down) 
           3. When a new partition is added topic


 Streaming API
 -------------

      1. Input KStream  --> receives the streaming data from one input topic
      2. I am splitting the input stream into three branches based on the key
      3. These three braches are streamed to three output topics


     order_log  ==> KStream ==>  branch[0] (k = 1)
                                 branch[1] (k = 2)  
                    		 branch[2] (k = 3)
			
    1. Producer is using custome partitioner
    2. Message is (key, value -> json string of my object)
    3. Streaming app to separate the data into three topics
    4. Consumer app is subscribing to 3 topics and priting the data.


  