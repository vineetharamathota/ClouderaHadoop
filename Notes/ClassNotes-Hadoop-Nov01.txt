
  Hive
  ----

   Hadoop -> HDFS + MapReduce + [ YARN + Zookeeper ]
   Hadoop Execution Frameworks => MapReduce, Spark, Tez
  
   Hive -> "Data Warehousing Framework" on Hadoop
        -> Uses HDFC as the Storage Layer
        -> Hive is a wrapper on top of MapReduce and exposes an SQL like interface to
           the devlopers, so that they need not write MR programs to perform data analysis.
           They can use an SQL-like language called "Hive Query Language"
   
    Developer => HQL => Hive => MR => Executed on Hadoop => Results are return as query output.

   
    Hive:
      
       Warehouse => Is a directory in HDFS, where hive stores all the data of its table.
                 => /user/hive/warehouse

       Metastore => Where hive stores all meta-data
                 => An external database is used (generally MySQL)


   Interacte with Hive:
       => CLI (Hive shell, Beeline Shell)
       => External Hive Clients (JDBC, ODBC, Thrift)
       => Hive User Environment (HUE)


   CLI => Hive Shell    (currently deprecated)
          Beeline Shell (Connects to hive using JDBC)
        
    
   Hive Warehouse:      
   ---------------

       Warehouse Path:  /user/hive/warehouse
                        /user/hive/warehouse/sdb.db

       => The 'default' database's is stored in warehouse directory itself. There
          won't be any dedicated directory for 'default' database. 

       => Every database have its own folder created inside warehouse directory

       => Every table has its own directory with in the database directory, where it stores
          all the tables data as files. 


  Loading Data into Hive
  ----------------------
  
     Copy data from local file system (Linux) to Hive warehouse: 

         LOAD DATA LOCAL INPATH '/home/cloudera/emp.txt' OVERWRITE INTO TABLE empinfo
         -> this is a copy op.

     Move data from HDFS to Hive warehouse:
         LOAD DATA INPATH '<hdfs file path>' OVERWRITE INTO TABLE <table name> 
         -> this is  move op.
     
  
  Schema on Write vs Schema on Read
  ---------------------------------

    => RDBMS validates data when the data is inserted into the DB, called 'Schema on Write'
    
    => Hive, follows 'Schema on Read'. 
        -> Schema validation happens when the data is read from the file
        -> There is no validation while loading the data.
   



  Two Types of Tables:

    1. Managed Tables
          -> The data is stored in Hive warehouse and are managed by Hive. 
          -> When a managed table is droped, both schema as well as data is deleted. 


    2. External Tables
         -> The data is referenced from an external location (other than hive warehouse)
         -> This could be pre-existing.
         -> Multiple tables/application can reference the same data
         -> Hive can not delete these data files. 
         -> When an external table is droped, only the meta data (from metastore) is deleted,
            but the actual data files are not deleted. 
   

   Partitioned Tables
   ==================

      => Structure of Data managed by Hive:

          Database => Tables => Table's data files
          Database => Table => Partitions => Partition's data files


     => Static Partitioning
           -> Is used when you know the partition into which you want to load the data.
           -> The partition value is given in the load statement.

     => Dynamic Partitioning
	   -> Is used when you have data related multiple partitions in the same file.
              In such a case, the file can not loaded into any specific patition.
           -> In Dynamic Partitioning, the data can not be loaded using "LOAD" statement,
              but the data should be loaded from a base table using "INSERT ... SELECT .." command.

   Bucketing:
   ==========

      1. When you have a column on which you want to create a partition, but this column
         is a "high-cardinality" column (such as name, id, email etc..), then your partition
         size tend to be very-low. You will end up with too many partitions and too small data 
         files. Here partitioning itself is not useful.

      2. When you have a column on which you want to partition, but your partition data size is 
         too-big (such as year), then you may think of adding an additional segmentation within each 
         partition (like split each partition in to 10 files based on some column).   
 
       
      Bucketing allows you to define a fixed number of segments based on a column value.  


	=> Structure of Data managed by Hive:

          	Database => Tables => Table's data files
          	Database => Table => Partitions => Partition's data files
		Database => Table => Buckets => data files  (on low-cardinality columns)
                Database => Table => Partitions => Buckets => data files


   Order By  vs.  Sort By
   ----------------------
                                               order by         sort by
    mapper ===> reducer 1  --> output file 1   (a, b, d)       (a, f, x)
                reducer 2  --> output file 1   (f, g, i)       (b, g, w)
                reducer 3  --> output file 1   (w, x, y)       (d, i, y) 

   
     Order By -> Global Sort -> data is sorted across partitions
     Sort By  -> Data is sorted with in each reducer


  Multi-table insert command
  ---------------------------

    from source_table
      insert into res_table_1 (col1, col2)
      select col1, col2 where ....     
      insert into res_table_2 (col1, col2)
      select col1, col2 where ....     
      insert into res_table_3 (col1, col2)
      select col1, col2 where ....



  File Formats
  ------------

   DEFAULT SERDE => DELIMITED  -> default delimiter:  Ctrl-A
   DEFAULT FORMAT => TEXTFILE

   CREATE TABLE mytable ( ........... )
   STORED AS SEQUENCEFILE 

   --- popular format ---
   CREATE TABLE mytable ( ........... )
   STORED AS ORC

   CREATE TABLE mytable ( ........... )
   STORED AS PARQUET

   CREATE TABLE mytable ( ........... )
   STORED AS AVRO



  Complex Data Types
  ------------------

    -> Array










   