  
  Sqoop    (SQL + Hadoop)
  -----

   => Is a structured data ingestion tool for Hadoop
   => Import / Export tool to move data to/from SQL Database & Hadoop

  
  
   1. sqoop help
   
   2. sqoop help <tool-name>   
      sqoop help import

   3. List databases, tables and running queries on MYSQL

  sqoop list-databases --connect jdbc:mysql://localhost:3306 --username root

  sqoop list-tables --connect jdbc:mysql://localhost:3306/empdb --username root

  sqoop eval --connect jdbc:mysql://localhost:3306/empdb --username root --query 'select * from emptable' 

  sqoop eval --connect jdbc:mysql://localhost:3306/empdb --username root --query 'update emptable set name="raju" where id = 1'

  sqoop eval --connect jdbc:mysql://localhost:3306/empdb --username root --query 'insert into emptable values(3, "suresh")'

  sqoop eval --connect jdbc:mysql://localhost:3306/empdb --username root --query 'delete from emptable where id = 3'
   
  4.   Import

sqoop import --connect jdbc:mysql://localhost:3306/empdb --username root --table emptable -m 1 --target-dir /user/cloudera/emptable_1 
sqoop import --connect jdbc:mysql://localhost/empdb --username root --table emptable --columns "name" -m 1 --target-dir /user/cloudera/empdb_cols
sqoop import --connect jdbc:mysql://localhost/empdb --username root --query "SELECT name,id FROM emptable WHERE id > 1 AND \$CONDITIONS" -m 1	--target-dir /user/cloudera/emptable_qry;
sqoop import --connect jdbc:mysql://localhost:3306/empdb --username root --table emptable --hive-table mydb.emp -m 1 --hive-import

  5. export

sqoop export --connect jdbc:mysql://localhost:3306/demo --table emptable3 --username root -m 1 --export-dir /user/cloudera/emptable_csv --input-fields-terminated-by ','


  6. create-hive-table

sqoop create-hive-table --connect jdbc:mysql://localhost:3306/empdb --username root --table emptable --fields-terminated-by ',' --hive-table mydb.emptable

  7. codegen

sqoop codegen  --connect jdbc:mysql://localhost:3306/empdb --table emp --class-name Employee


==================================================

  Oozie
  -----

   ==> Apache NiFi, Apache AirFlow, Apache Oozie

   1. MR Programs                    ---> output
   2. Pig Latin Scripts              ---> output
   3. Load these outputs into Hive   ---> Hive Summary Table
   4. Send out an email              --->
   5. Export the data from Hive Summary table to MySQL 

        --> create a workflow out of these steps                    (WorkFlow)
        --> run that workflow on a schedule (every day at 1:00 AM ) (Coordinator)

   1. Map Reduce   --> Multiple MR programs
   2. Hive Script  --> a script with multiple hive step.

  
   OOZIE  =>   1. WorkFlow     ( DAG of Actions )
               2. Coordinator  ( Schedule the workflows )
               3. Bundle       ( Define Data-pipelines )


 -----------------------------------------------------------

   Dataset: pusdudo_facebook.csv

   Requirment: Find the record with highest likes from amoung the people who were born between
              1990 and 2000.

       => MR 1 -> Filter all the records where year between 1990 and 2000. (Age)
       => MR 2 -> Find the record with max-likes. (Likes)
   
   facebook.tsv  ==> AgeJob ==> Output (1990-2000) ==> LikesJob ==> HDFS

   hdfs://localhost.localdomain:8020/user/cloudera/output-data/facebook-mapred-age-output

   ${nameNode}/user/${user.name}/${ageOutputDir}

--------------------------------------------------------------  

  HBase
  -----

   => 2003 and 2006


    Google File System   --> Hadoop HDFS       (Distributed Storage)
    MapReduce            --> Hadoop MapReduce  (Ditributed Processing)         

    Google Big Table     --> HBase             (Distributed Transactional Processing)
  

  Limitations of Hadoop related to transactional processing:
  ----------------------------------------------------------

  1. Unstructured Data
  2. No Random Access
  3. High Latencies
  4. No ACID complaint --> RDBMS Guarentee. 


  HBase:
  -----
  1. Distributed database management system that is part of Hadoop ecosystem.
  2. Based "Google BigTable"
  3. Uses HDFS to store data:  Distributed Storage, Fault-tolerant.


  HBase vs RDBMS
  --------------

   -> HBase does not support SQL. It is a NoSQL database.
   -> HBase only supports a basic set of operations (CRUD) on a "single row".
   -> All operations are applied at "row-level"
   -> No JOINS, GROUPING, SUB-QUERIES.
   -> There is np regid schema - columns are not defined while creating a table.
 
   -> Is a "Column Family" oriented database. (Distributed Sorted Maps)

      -> works at 'cell' level
      -> Each cell represents a data point  ( row is the datapoint in a table )
     
      -> HBase stores data as a map (sorted-maps)
          
            key: < Row-Id, Col-Id >
            value: Value of the cell

            MySQL => <id, name, age> => 1, Raju, 45

            HBase =>  1, "id", 1               <=  (RowId, Column-name, Column-value)
                      1, "name", "Raju"
                      1, "age", 45

                      2, "id", 2
                      2, "name", "Sameer"
                      2, "age", 28
                      2, "email", "sameer@gmail.com"

    -> Denormalized 

           MySQL
           ======
           EmpBasics            => empid, name, phone, email, dob, gender, managerid  
           EmpAddress           => addressid, empid, address
           EmployeeSubordinates => esid, empid, subordinateid

           HBase
           ======
           Employee => basics, address, subordinates  (column-families)
 
                  1, "basics:name", "Raju"
                  1, "baics:phone", "342423423423"
                  1, "address:homeaddress", "...."
                  1, "suborinate:sub1", "name1"

          Normalized design is optimized for "storage"
          Denormalized design is optimized for "disk seeks" (storage is cheap)



    Facebook Message
    ----------------
       A friend request  -> one user
       Comment
       Like
                   

    HBase vs Hive
    -------------
 

   HBase Data Model
   ----------------

    Table 
	=> Row 
	     => Column-familes
		 => Columns
		      => Cell (Value + TimeStamp)


   HBase Architecture
   ------------------
	
    In HBase, table is a "sorted nested map"

    < Row-Id, 
        Column-family.<column, <timestamp, value>>

   -> When you read data, it perform a lookup based on the row-id.
   -> When you write data, it needs to insert the data in the right place, so that rows are sorted. 
      -> with the help of Region Servers.
   
    
   	      
HBase Cluster  => HMaster (master) & RegionServers (slave daemons)
                  => allocates regions to region servers.

               => Some regions are allocated to region servers.
                 


 RS1 -> R1, R3, R4, R6  (like an index for fast lookup)
 RS2 -> R2, R5, R7, R8



1  -> region (contiguous set of rows)
2
3

4  -> region
5
6

7  -> region
8
9
10
     
  Region Server => [MemStore] (RAM); 
                   [WriteAhead-Log] (disk)  -> used for recovery
                   [HFiles]  --> HDFS

      
  -> All writes are stored in RAM (MemStores)
  -> Whenever there is a change, the data is updated in the MemStore, a change-log is written
     to disk
  -> Periodically the MemSTore gets full, HFiles are written to HDFS
  -> Keeps an "index of row-key to HFile block" in memory
   


 When you read/write:

  1. RS containing the row-key is identify
  2. RS will see the data in MemFile and HFiles.







